{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by:  Victor Christoffersson, 9312155576, vicchri@student.chalmers.se & Jacob Lundberg, 9409249233, jacobtu@student.chalmers.se ** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## Solution\n",
    "\n",
    "### 1.\n",
    "The formula for naive bayes is the following:\n",
    "$$ \n",
    "P( content | \\neg rich, married, healthy) = \\frac{1}{Z} * P(content) * P(\\neg rich|content) * P(married|content) * P(healthy|content)\n",
    "$$\n",
    "\n",
    "With\n",
    "$ \n",
    "Z = \\Sigma_k \\left( P(C_k) * P(\\neg rich,married,healthy|C_k) \\right), C_k { \\in content, \\neg content }\n",
    "$\n",
    "\n",
    "\n",
    "$$P(content) = \\frac{4}{8}\n",
    "$$\n",
    "\n",
    "$$P(~rich,married,healthy|content) = P(~rich|content) * P(married|content) * P(healthy|content) = \\frac{1}{4} * \\frac{2}{4} * \\frac{3}{4} = \\frac{3}{32}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = P(content)* p(\\neg rich|content) * P(married|content) * P(healthy|content) + P(\\neg content) * p(\\neg rich|\\neg content) * P(married|\\neg content) * P(healthy|\\neg content) = \\frac{4}{8} * \\frac{3}{32} + \\frac{4}{8} * \\frac{3}{4} * \\frac{1}{4} * \\frac{1}{4} = \\frac{9}{128}\n",
    "$$\n",
    "\n",
    "Finally, we get:\n",
    "$$\n",
    "P(content|\\neg rich,married,healthy) = \\frac{1}{\\frac{9}{128}} * \\frac{4}{8} * \\frac{3}{32} = \\frac{2}{3}\n",
    "$$\n",
    "### 2.\n",
    "Here we just need to use the same rule as above except we don't need to care about healthy since it can be both 0 and 1\n",
    "\n",
    "$$ \n",
    "P( content | \\neg rich, married) = \\frac{1}{Z} * P(content) * P(\\neg rich|content) * P(married|content)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(content) = \\frac{4}{8}\n",
    "$$\n",
    "\n",
    "$$P(\\neg rich,married|content) = P(\\neg rich|content) * P(married|content) = \\frac{1}{4} * \\frac{2}{4} * = \\frac{1}{8}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = P(content)* p(\\neg rich|content) * P(married|content) + P(\\neg content) * p(\\neg rich|\\neg content) * P(married|\\neg content) = \\frac{1}{16} + \\frac{3}{32} = \\frac{5}{32}\n",
    "$$\n",
    "\n",
    "And finally:\n",
    "$$\n",
    "P(content|\\neg rich,married) = \\frac{1}{\\frac{5}{32}} * \\frac{4}{8} * \\frac{1}{8} = \\frac{2}{5}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n",
    "\n",
    "## Solution\n",
    "$x_1, x_2, x_3$ are not independent of each other, they are constrained by $x_1 + x_2 + x_3 = 1$\n",
    "you can instead use one variable for $x_1, x_2, x_3$ that can take the value $0, 1, 2$ respectively and then use naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n",
    "\n",
    "## Solution\n",
    "\n",
    "### a.\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) = \\frac{P(y_{new} = -1) * P(x_{new}| y_{new} = -1, X, y)}{P(x_{new}, X, y)} = \\frac{P(y_{new} = -1) * P(x_{new}| y_{new} = -1, X, y)}{\\Sigma_j P(x_{new}| y_{new} = j, X, y)P(y_{new} = j)} \\\\\n",
    "$$\n",
    "$$\n",
    "P(y_{new} = +1 | x_{new} , X, y ) = \\frac{P(y_{new} = +1) * P(x_{new}| y_{new} = +1, X, y)}{P(x_{new}, X, y)} = \\frac{P(y_{new} = +1) * P(x_{new}| y_{new} = +1, X, y)}{\\Sigma_j P(x_{new}| y_{new} = j, X, y)P(y_{new} = j)}\n",
    "$$\n",
    "In this case they can be simplified using the following:\n",
    "$$P(y_{new} = -1) = \\frac{1}{2} \\\\ P(y_{new} = +1) = \\frac{1}{2} ~.$$\n",
    "\n",
    "$$P(x_{new}| y_{new} = +1, X, y) = P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})  \\\\\n",
    "P(x_{new}| y_{new} = -1, X, y) = P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2}) ~. $$\n",
    "\n",
    "$$\n",
    "\\Sigma_j P(x_{new}| y_{new} = j, X, y)P(y_{new} = j) = \\frac{1}{2} * (P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1}) + P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})) \n",
    "$$\n",
    "\n",
    "plug these into the two formulas:\n",
    "$$\n",
    "P(y_{new} = +1 | x_{new} , X, y ) = \\frac{P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})}{P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1}) + P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})} \\\\\n",
    "P(y_{new} = -1 | x_{new} , X, y ) = \\frac{P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})}{P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1}) + P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})} \\\\\n",
    "~.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error for sph_bayes:  0.0\n",
      "Percent error for new_classifer:  0.0\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import model_selection\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "\n",
    "labels1 = data[:1000,:-1]\n",
    "labels2 = data[1000:2000,:-1]\n",
    "data1 = data[:1000,:3]\n",
    "data2 = data[1000:2000,:3]\n",
    "\n",
    "#mean\n",
    "mu1 = np.mean(data1,axis=0)\n",
    "mu2 = np.mean(data2,axis=0)\n",
    "\n",
    "#var\n",
    "sigma1 = np.cov(data1,rowvar=False)\n",
    "sigma2 = np.cov(data2,rowvar=False)\n",
    "\n",
    "def sph_bayes(Xtest, mu1, mu2, sigma1, sigma2):\n",
    "    mvn1 = multivariate_normal(mu1,sigma1)\n",
    "    p1 = mvn1.pdf(Xtest)\n",
    "    mvn2 = multivariate_normal(mu2,sigma2)\n",
    "    p2 = mvn2.pdf(Xtest)\n",
    "    \n",
    "    P1 = p1 / (p1 + p2)\n",
    "    P2 = p2 / (p1 + p2)\n",
    "    \n",
    "    if(P1 > P2):\n",
    "        Ytest = 1\n",
    "    else:\n",
    "        Ytest = -1\n",
    "    return [P1, P2, Ytest]\n",
    "\n",
    "def new_classifier(Xtest, mu1, mu2):\n",
    "    b = (mu1+mu2)/2\n",
    "    muDiff = mu1-mu2\n",
    "    sign = np.sign(muDiff.T.dot(Xtest-b) / np.linalg.norm(muDiff))\n",
    "    Ytest = sign\n",
    "    return [Ytest]\n",
    "\n",
    "def kfold(folds,data1,data2):\n",
    "    kf = model_selection.KFold(folds, shuffle=True)\n",
    "    split = kf.split(data1)\n",
    "    \n",
    "    errnc = 0\n",
    "    errsb = 0\n",
    "    for train_index, test_index in kf.split(data1):\n",
    "        Xpos_train, Xpos_test = data1[train_index], data1[test_index]\n",
    "        Xneg_train, Xneg_test = data2[train_index], data2[test_index]\n",
    "        mu1 = np.mean(Xpos_train, axis=0)\n",
    "        mu2 = np.mean(Xneg_train, axis=0)\n",
    "        \n",
    "        sigma1 = np.cov(Xpos_train,rowvar=False)\n",
    "        sigma2 = np.cov(Xneg_train,rowvar=False)\n",
    "        \n",
    "        for xtest in Xpos_test:\n",
    "            ync = new_classifier(xtest,mu1,mu2)\n",
    "            _,_,ysb = sph_bayes(xtest, mu1, mu2, sigma1, sigma2)\n",
    "\n",
    "            if(ysb!=1): \n",
    "                errsb += 1\n",
    "            if(ync[0]!=1): \n",
    "                errnc += 1\n",
    "        \n",
    "        for xtest in Xneg_test:\n",
    "            ync = new_classifier(xtest,mu1,mu2)\n",
    "            _,_,ysb = sph_bayes(xtest, mu1, mu2, sigma1, sigma2)\n",
    "\n",
    "            if(ysb==1): \n",
    "                errsb += 1\n",
    "            if(ync[0]==1): \n",
    "                errnc += 1    \n",
    "    \n",
    "    tests = (data1.shape[0] + data2.shape[0])\n",
    "    return [errsb / tests,errnc / tests]\n",
    "    \n",
    "Xtest = np.array([0.3,0.5,0.3])\n",
    "sph_bayes(Xtest, mu1, mu2, sigma1, sigma2)\n",
    "new_classifier(Xtest,mu1,mu2)\n",
    "sb, nc = kfold(5,data1,data2)\n",
    "print(\"Percent error for sph_bayes: \", sb*100)\n",
    "print(\"Percent error for new_classifer: \", nc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?\n",
    "\n",
    "### Solution\n",
    "In the test we get that the results based on the mean gives about 1.7% error while the ones based on variance gives about 13.5% error, which is a significantly worse result. The scaling of the vector should not affect the result, which means that the variance of rows and columns does much worse at classifying than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error for mean data:  1.6853932584269662\n",
      "Percent error for variance data:  1.6853932584269662\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "data5 = data[digits.target == 5]\n",
    "data8 = data[digits.target == 8]\n",
    "\n",
    "#get data to check\n",
    "dat = data[(digits.target == 5) | (digits.target == 8)]\n",
    "#get values to check against test\n",
    "check = digits.target[(digits.target == 5) | (digits.target == 8)]\n",
    "\n",
    "def classify_digits(xtest,mu1,mu2):\n",
    "    y = new_classifier(xtest,mu1,mu2)\n",
    "    return y[0]\n",
    "\n",
    "def var_features(data):\n",
    "    data = data / 255\n",
    "    n = data.shape[0]\n",
    "    var_data = np.empty(shape=(n,16))\n",
    "    for obs in range(0,n):\n",
    "        for i in range(0,8):\n",
    "            var = np.var(data[obs][(i)*8:(i+1)*8])\n",
    "            var_data[obs][i] = np.var(data[obs][(i)*8:(i+1)*8])\n",
    "            var_data[obs][i+8] = np.var(data[obs][i::8])\n",
    "    return var_data\n",
    "   \n",
    "def kfold_img(folds,data,check):\n",
    "    kf = model_selection.KFold(5, shuffle=True)\n",
    "    \n",
    "    errnc = 0\n",
    "    errvf = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(data):        \n",
    "        X_train, X_test = data[train_index], data[test_index]      \n",
    "        Y_train, Y_test = check[train_index], check[test_index]\n",
    "\n",
    "        X5_train = X_train[Y_train == 5]\n",
    "        X8_train = X_train[Y_train == 8]\n",
    "        mu5 = np.mean(X5_train, axis=0)\n",
    "        mu8 = np.mean(X8_train, axis=0)  \n",
    "\n",
    "        X_var = var_features(X_train)\n",
    "        X_test_var = var_features(X_test)\n",
    "        X5_var = var_features(X5_train)\n",
    "        X8_var = var_features(X8_train)\n",
    "        mu5_var = np.mean(X5_var, axis=0)\n",
    "        mu8_var = np.mean(X8_var, axis=0)\n",
    "        \n",
    "        i = 0\n",
    "        for xtest in X_test:\n",
    "            nc = classify_digits(xtest,mu5,mu8)\n",
    "            if nc == 1:\n",
    "                x_check = 5\n",
    "            else:\n",
    "                x_check = 8\n",
    "            if x_check != Y_test[i]:\n",
    "                errnc += 1\n",
    "            i+=1\n",
    "        \n",
    "        j = 0\n",
    "        for xtest in X_test_var:\n",
    "            vf = classify_digits(xtest,mu5_var,mu8_var)\n",
    "            if vf == 1:\n",
    "                x_check = 5\n",
    "            else:\n",
    "                x_check = 8\n",
    "            if x_check != Y_test[j]:\n",
    "                errvf += 1\n",
    "            j+=1\n",
    "\n",
    "                \n",
    "    tests = data.shape[0]\n",
    "    return [errnc / tests,errvf / tests]    \n",
    "    \n",
    "\n",
    "nc,vf = kfold_img(5,dat,check)\n",
    "print(\"Percent error for mean data: \", nc*100)\n",
    "print(\"Percent error for variance data: \", vf*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
