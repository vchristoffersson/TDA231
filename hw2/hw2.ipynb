{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by:  Victor Christoffersson, 9312155576, vicchri@student.chalmers.se & Jacob Lundberg, 9409249233, jacobtu@student.chalmers.se ** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## Solution\n",
    "\n",
    "### 1.\n",
    "\n",
    "\n",
    "create likelihood table from the data:\n",
    "         total | given content \n",
    "~rich:   4/8     1/4 \n",
    "married: 3/8     2/4\n",
    "healthy: 4/8     3/4\n",
    "content: 4/8\n",
    "         \n",
    "Use bayes to calculate posterior prob for each class\n",
    "\n",
    "Posterior = prior * likelihood / evidence\n",
    "\n",
    "we want P(content|~rich,married,healthy) = P(~rich,married,healthy|content) * P(content) / P(~rich,married,healthy) \n",
    "which because of independence is equal to 1/Z * P(content) * P(~rich|content) * P(married|content) * P(healthy|content)\n",
    "\n",
    "P(~rich,married,healthy|content) = P(~rich|content) * P(married|content) * P(healthy|content) = 1/4 * 2/4 * 3/4 = 3/32\n",
    "\n",
    "P(content) = 4/8\n",
    "\n",
    "Z = P(content)* p(~rich|content) * P(married|content) * P(healthy|content) + P(~content) * p(~rich|~content) * P(married|~content) * P(healthy|~content) = 4/8 * 3/32 + 4/8 * 3/4 * 1/4 * 1/4 = 9/128\n",
    "\n",
    "using this we get: 3/32 * 4/8 / 9/128 = 2/3\n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "\n",
    "### 2.\n",
    "Just use the same rules as above except don't factor in healthiness, works because of independence\n",
    "\n",
    "P(content|~rich,married) = P(~rich,married|content) * P(content) / P(~rich,married) = 1/Z * P(content) * P(~rich|content) * P(married|content)\n",
    "\n",
    "P(~rich,married|content) = P(~rich|content) * P(married|content) = 1/4 * 2/4 = 1/8\n",
    "\n",
    "P(content) = 4/8\n",
    "\n",
    "With Z = P(content)* p(~rich|content) * P(married|content) + P(~content) * p(~rich|~content) * P(married|~content) = 1/16 + 3/32 = 5/32\n",
    "\n",
    "and finally: 1/8 * 4/8 / 5/32 = 2/5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n",
    "\n",
    "X_1, x_2 and x_3 are not independent of each other, they are constrained by x_1 + x_2 + x_3 = 1\n",
    "you can instead extend naive bayes to use one variable for x_1, x_2 and x_3 that can take the value 0, 1 and 2 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n",
    "\n",
    "## Solution\n",
    "\n",
    "### a.\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) = \\frac{P(y_{new} = -1) * P(x_{new}| y_{new} = -1, X, y)}{P(x_{new}, X, y)} = \\frac{P(y_{new} = -1) * P(x_{new}| y_{new} = -1, X, y)}{\\Sigma_j P(x_{new}| y_{new} = j, X, y)P(y_{new} = j)} \\\\\n",
    "$$\n",
    "$$\n",
    "P(y_{new} = +1 | x_{new} , X, y ) = \\frac{P(y_{new} = +1) * P(x_{new}| y_{new} = +1, X, y)}{P(x_{new}, X, y)} = \\frac{P(y_{new} = +1) * P(x_{new}| y_{new} = +1, X, y)}{\\Sigma_j P(x_{new}| y_{new} = j, X, y)P(y_{new} = j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error for sph_bayes:  16.28125\n",
      "Percent error for new_classifer:  0.0\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import model_selection\n",
    "#from sklearn.model_selection import KFold\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "\n",
    "labels1 = data[:1000,:-1]\n",
    "labels2 = data[1000:2000,:-1]\n",
    "data1 = data[:1000,:3]\n",
    "data2 = data[1000:2000,:3]\n",
    "\n",
    "#mean\n",
    "mu1 = np.mean(data1,axis=0)\n",
    "mu2 = np.mean(data2,axis=0)\n",
    "\n",
    "#var\n",
    "sigma1 = np.cov(data1,rowvar=False)\n",
    "sigma2 = np.cov(data2,rowvar=False)\n",
    "\n",
    "def sph_bayes(Xtest, mu1, mu2, sigma1, sigma2):\n",
    "    mvn1 = multivariate_normal(mu1,sigma1)\n",
    "    p1 = mvn1.pdf(Xtest)\n",
    "    mvn2 = multivariate_normal(mu2,sigma2)\n",
    "    p2 = mvn2.pdf(Xtest)\n",
    "    \n",
    "    P1 = p1 / (p1 + p2)\n",
    "    P2 = p2 / (p1 + p2)\n",
    "    \n",
    "    if(P1 > P2):\n",
    "        Ytest = 1\n",
    "    else:\n",
    "        Ytest = -1\n",
    "    return [P1, P2, Ytest]\n",
    "\n",
    "def new_classifier(Xtest, mu1, mu2):\n",
    "    b = (mu1+mu2)/2\n",
    "    muDiff = mu1-mu2\n",
    "    sign = np.sign(muDiff.T.dot(Xtest-b) / np.linalg.norm(muDiff))\n",
    "    Ytest = sign\n",
    "    return [Ytest]\n",
    "\n",
    "def kfold(folds,data1,data2):\n",
    "    kf = model_selection.KFold(folds, shuffle=True)\n",
    "    split = kf.split(data1)\n",
    "    \n",
    "    errnc = 0\n",
    "    errsb = 0\n",
    "    for train_index, test_index in kf.split(data1):\n",
    "        Xpos_train, Xpos_test = data1[train_index], data1[test_index]\n",
    "        Xneg_train, Xneg_test = data2[train_index], data2[test_index]\n",
    "        mu1 = np.mean(Xpos_train, axis=0)\n",
    "        mu2 = np.mean(Xneg_train, axis=0)\n",
    "        \n",
    "        sigma1 = np.cov(Xpos_train,rowvar=False)\n",
    "        sigma2 = np.cov(Xneg_train,rowvar=False)\n",
    "        \n",
    "        for xtest in Xpos_test:\n",
    "            ync = new_classifier(xtest,mu1,mu2)\n",
    "            ysb = sph_bayes(xtest, mu1, mu2, sigma1, sigma2)\n",
    "\n",
    "            if(ysb[0]!=1): \n",
    "                errsb += 1\n",
    "            if(ync[0]!=1): \n",
    "                errnc += 1\n",
    "        \n",
    "        for xtest in Xneg_test:\n",
    "            ync = new_classifier(xtest,mu1,mu2)\n",
    "            ysb = sph_bayes(xtest, mu1, mu2, sigma1, sigma2)\n",
    "\n",
    "            if(ysb[0]==1): \n",
    "                errsb += 1\n",
    "            if(ync[0]==1): \n",
    "                errnc += 1    \n",
    "    \n",
    "    tests = (Xpos_test.shape[0] + Xneg_test.shape[0]) * (folds-1) * 2\n",
    "    return [errsb / tests,errnc / tests]\n",
    "    \n",
    "Xtest = np.array([0.3,0.5,0.3])\n",
    "sph_bayes(Xtest, mu1, mu2, sigma1, sigma2)\n",
    "new_classifier(Xtest,mu1,mu2)\n",
    "sb, nc = kfold(5,data1,data2)\n",
    "print(\"Percent error for sph_bayes: \", sb*100)\n",
    "print(\"Percent error for new_classifer: \", nc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error for mean data:  32.04225352112676\n",
      "Percent error for variance data:  32.04225352112676\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "data5 = data[digits.target == 5]\n",
    "data8 = data[digits.target == 8]\n",
    "\n",
    "#get data to check\n",
    "dat = data[(digits.target == 5) | (digits.target == 8)]\n",
    "#get values to check against test\n",
    "check = digits.target[(digits.target == 5) | (digits.target == 8)]\n",
    "\n",
    "def classify_digits(xtest,mu1,mu2):\n",
    "    return new_classifier(xtest,mu1,mu2)\n",
    "\n",
    "def var_features(data):\n",
    "    data = data / 255\n",
    "    n = data.shape[0]\n",
    "    var_data = np.empty(shape=(n,16))\n",
    "    for obs in range(0,n):\n",
    "        for i in range(0,8):\n",
    "            var = np.var(data[obs][(i)*8:(i+1)*8])\n",
    "            var_data[obs][i] = np.var(data[obs][(i)*8:(i+1)*8])\n",
    "            var_data[obs][i+8] = np.var(data[obs][i::8])\n",
    "    return var_data\n",
    "   \n",
    "def kfold_img(folds,data,check):\n",
    "    kf = model_selection.KFold(5, shuffle=True)\n",
    "    \n",
    "    errnc = 0\n",
    "    errvf = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(data):        \n",
    "        X_train, X_test = data[train_index], data[test_index]      \n",
    "        Y_train, Y_test = check[train_index], check[test_index]\n",
    "\n",
    "        X5_train = X_train[Y_train == 5]\n",
    "        X8_train = X_train[Y_train == 8]\n",
    "        mu5 = np.mean(X5_train, axis=0)\n",
    "        mu8 = np.mean(X8_train, axis=0)  \n",
    "\n",
    "        X_var = var_features(X_train)\n",
    "        X_test_var = var_features(X_test)\n",
    "        X5_var = var_features(X5_train)\n",
    "        X8_var = var_features(X8_train)\n",
    "        mu5_var = np.mean(X5_var, axis=0)\n",
    "        mu8_var = np.mean(X8_var, axis=0)\n",
    "        \n",
    "        i = 0\n",
    "        for xtest in X_test:\n",
    "            nc = classify_digits(xtest,mu5,mu8)\n",
    "            if nc == 1:\n",
    "                x_check = 5\n",
    "            else:\n",
    "                x_check = 8\n",
    "            if x_check != Y_test[i]:\n",
    "                errnc += 1\n",
    "            i+=1\n",
    "        \n",
    "        j = 0\n",
    "        for xtest in X_test_var:\n",
    "            vf = classify_digits(xtest,mu5_var,mu8_var)\n",
    "            if vf == 1:\n",
    "                x_check = 5\n",
    "            else:\n",
    "                x_check = 8\n",
    "            if x_check != Y_test[j]:\n",
    "                errvf += 1\n",
    "            j+=1\n",
    "\n",
    "                \n",
    "    tests = X_test.shape[0] * (folds-1) * 2\n",
    "    return [errnc / tests,errvf / tests]    \n",
    "    \n",
    "\n",
    "nc,vf = kfold_img(5,dat,check)\n",
    "print(\"Percent error for mean data: \", nc*100)\n",
    "print(\"Percent error for variance data: \", vf*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
